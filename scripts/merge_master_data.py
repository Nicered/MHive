#!/usr/bin/env python3
"""
ë§ˆìŠ¤í„° ë°ì´í„° í†µí•© ìŠ¤í¬ë¦½íŠ¸
ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë§ˆìŠ¤í„° ë°ì´í„°ë¡œ í†µí•©í•©ë‹ˆë‹¤.
"""

import json
import os
from typing import List, Dict, Set, Tuple
from datetime import datetime
from tqdm import tqdm
import hashlib

SCRIPTS_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(SCRIPTS_DIR, '..', 'data')
SOURCES_DIR = os.path.join(DATA_DIR, 'sources')
OUTPUT_DIR = os.path.join(DATA_DIR, 'master')

# ë°ì´í„° ì†ŒìŠ¤ ì„¤ì •
DATA_SOURCES = [
    {
        'file': 'fema_disasters.json',
        'name': 'OpenFEMA',
        'priority': 1,  # ìš°ì„ ìˆœìœ„ (ë‚®ì„ìˆ˜ë¡ ë†’ìŒ)
    },
    {
        'file': 'usgs_earthquakes.json',
        'name': 'USGS',
        'priority': 2,
    },
    {
        'file': 'noaa_tsunamis.json',
        'name': 'NOAA',
        'priority': 3,
    },
    {
        'file': 'coldcases_crimes.json',
        'name': 'Harvard-LIL',
        'priority': 4,
    },
]


def load_source_data(filename: str) -> List[Dict]:
    """ì†ŒìŠ¤ íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    filepath = os.path.join(SOURCES_DIR, filename)
    if not os.path.exists(filepath):
        print(f"  âš ï¸  íŒŒì¼ ì—†ìŒ: {filename}")
        return []

    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('incidents', [])
    except Exception as e:
        print(f"  âŒ ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
        return []


def generate_dedup_key(incident: Dict) -> str:
    """ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # ë‚ ì§œ + ìœ„ì¹˜ + ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í‚¤
    date = (incident.get('date') or '')[:10]  # YYYY-MM-DD
    location = (incident.get('location') or '').lower()[:30]
    category = incident.get('category') or ''

    # ì¢Œí‘œ ê¸°ë°˜ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
    coords = incident.get('coordinates')
    if coords:
        lat = round(coords.get('lat', 0), 1)
        lng = round(coords.get('lng', 0), 1)
        coord_str = f"{lat},{lng}"
    else:
        coord_str = ""

    key_str = f"{date}|{location}|{category}|{coord_str}"
    return hashlib.md5(key_str.encode()).hexdigest()[:16]


def normalize_incident(incident: Dict, source_name: str) -> Dict:
    """ì¸ì‹œë˜íŠ¸ ë°ì´í„°ë¥¼ ì •ê·œí™”í•©ë‹ˆë‹¤."""
    # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ê¸°ë³¸ê°’ ì„¤ì •
    normalized = {
        'id': 0,  # ë‚˜ì¤‘ì— ì¬í• ë‹¹
        'title': incident.get('title', 'Unknown Incident'),
        'category': incident.get('category', 'disaster'),
        'era': incident.get('era', 'contemporary'),
        'date': incident.get('date', ''),
        'location': incident.get('location', 'Unknown'),
        'summary': incident.get('summary', '')[:400],
        'description': incident.get('description', '')[:3000],
        'timeline': incident.get('timeline', []),
        'theories': incident.get('theories', []),
        'tags': incident.get('tags', []),
        'sources': incident.get('sources', []),
        'relatedIncidents': [],  # ë‚˜ì¤‘ì— ê³„ì‚°
        'images': incident.get('images', []),
        'casualties': incident.get('casualties'),
        'coordinates': incident.get('coordinates'),
        'status': incident.get('status', 'resolved'),
    }

    # ì†ŒìŠ¤ ì •ë³´ ì¶”ê°€
    normalized['_source'] = source_name
    normalized['_originalId'] = incident.get('originalId', incident.get('id'))

    # ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ìˆëŠ” ê²½ìš°)
    for key in ['magnitude', 'depth', 'maxWaveHeight', 'femaDisasterNumber']:
        if key in incident:
            normalized[key] = incident[key]

    return normalized


def merge_duplicates(incidents: List[Dict]) -> List[Dict]:
    """ì¤‘ë³µ ë°ì´í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤."""
    dedup_map = {}  # key -> list of incidents

    for inc in incidents:
        key = generate_dedup_key(inc)
        if key not in dedup_map:
            dedup_map[key] = []
        dedup_map[key].append(inc)

    merged = []
    for key, group in dedup_map.items():
        if len(group) == 1:
            merged.append(group[0])
        else:
            # ê°€ì¥ ì •ë³´ê°€ í’ë¶€í•œ ê²ƒì„ ì„ íƒ
            best = max(group, key=lambda x: (
                len(x.get('description', '')),
                len(x.get('tags', [])),
                1 if x.get('coordinates') else 0,
                1 if x.get('casualties') else 0,
            ))

            # ë‹¤ë¥¸ ì†ŒìŠ¤ ì •ë³´ ë³‘í•©
            all_sources = []
            all_tags = set(best.get('tags', []))
            for inc in group:
                all_sources.extend(inc.get('sources', []))
                all_tags.update(inc.get('tags', []))

            # ì¤‘ë³µ ì†ŒìŠ¤ ì œê±°
            seen_urls = set()
            unique_sources = []
            for src in all_sources:
                url = src.get('url', '')
                if url not in seen_urls:
                    seen_urls.add(url)
                    unique_sources.append(src)

            best['sources'] = unique_sources
            best['tags'] = list(all_tags)[:15]  # ìµœëŒ€ 15ê°œ íƒœê·¸

            merged.append(best)

    return merged


def calculate_relations(incidents: List[Dict], max_relations: int = 5) -> List[Dict]:
    """ì¸ì‹œë˜íŠ¸ ê°„ì˜ ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    relations = []

    # ì¸ë±ìŠ¤ ìƒì„±
    by_category = {}
    by_year = {}
    by_tag = {}

    for inc in incidents:
        inc_id = inc['id']
        cat = inc.get('category', '')
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(inc_id)

        try:
            year = int(inc.get('date', '2000')[:4])
            if year not in by_year:
                by_year[year] = []
            by_year[year].append(inc_id)
        except:
            pass

        for tag in inc.get('tags', []):
            if tag not in by_tag:
                by_tag[tag] = []
            by_tag[tag].append(inc_id)

    # ê´€ê³„ ê³„ì‚°
    for inc in tqdm(incidents, desc="ê´€ê³„ ê³„ì‚°"):
        inc_id = inc['id']
        scores = {}  # other_id -> score

        cat = inc.get('category', '')
        try:
            year = int(inc.get('date', '2000')[:4])
        except:
            year = 2000

        # ê°™ì€ ì¹´í…Œê³ ë¦¬ (+2)
        for other_id in by_category.get(cat, []):
            if other_id != inc_id:
                scores[other_id] = scores.get(other_id, 0) + 2

        # ë¹„ìŠ·í•œ ì‹œê¸° (+1~3)
        for y in range(year - 2, year + 3):
            weight = 3 if y == year else (2 if abs(y - year) == 1 else 1)
            for other_id in by_year.get(y, []):
                if other_id != inc_id:
                    scores[other_id] = scores.get(other_id, 0) + weight

        # ê³µí†µ íƒœê·¸ (+1 per tag)
        for tag in inc.get('tags', []):
            for other_id in by_tag.get(tag, []):
                if other_id != inc_id:
                    scores[other_id] = scores.get(other_id, 0) + 1

        # ìƒìœ„ ê´€ê³„ë§Œ ì„ íƒ
        top_relations = sorted(scores.items(), key=lambda x: -x[1])[:max_relations]
        inc['relatedIncidents'] = [r[0] for r in top_relations if r[1] >= 3]

        # ê´€ê³„ ë°ì´í„° ìƒì„±
        for other_id, score in top_relations:
            if score >= 3 and inc_id < other_id:  # ì¤‘ë³µ ë°©ì§€
                relation_type = "ê´€ë ¨ ì‚¬ê±´"
                if cat == incidents[other_id - 1].get('category'):
                    relation_type = f"ë™ì¼ ìœ í˜• ({cat})"
                relations.append({
                    'from': inc_id,
                    'to': other_id,
                    'relation': relation_type,
                    'score': score,
                })

    return relations


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”„ ë§ˆìŠ¤í„° ë°ì´í„° í†µí•© ì‹œì‘...")
    print("=" * 60)

    all_incidents = []

    # ê° ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ
    for source in DATA_SOURCES:
        print(f"\nğŸ“‚ {source['name']} ë°ì´í„° ë¡œë“œ ì¤‘...")
        incidents = load_source_data(source['file'])
        print(f"   ë¡œë“œë¨: {len(incidents)}ê°œ")

        # ì •ê·œí™”
        for inc in incidents:
            normalized = normalize_incident(inc, source['name'])
            all_incidents.append(normalized)

    print(f"\nğŸ“Š ì´ ë¡œë“œëœ ë°ì´í„°: {len(all_incidents)}ê°œ")

    # ì¤‘ë³µ ì œê±° ë° ë³‘í•©
    print("\nğŸ” ì¤‘ë³µ ë°ì´í„° ë³‘í•© ì¤‘...")
    merged = merge_duplicates(all_incidents)
    print(f"   ë³‘í•© í›„: {len(merged)}ê°œ")

    # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
    merged.sort(key=lambda x: x.get('date', ''), reverse=True)

    # ID ì¬í• ë‹¹
    for i, inc in enumerate(merged):
        inc['id'] = i + 1

    # ê´€ê³„ ê³„ì‚°
    print("\nğŸ”— ê´€ê³„ ê³„ì‚° ì¤‘...")
    relations = calculate_relations(merged)
    print(f"   ìƒì„±ëœ ê´€ê³„: {len(relations)}ê°œ")

    # ê²°ê³¼ ì €ì¥
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ë§ˆìŠ¤í„° ë°ì´í„° ì €ì¥
    master_file = os.path.join(OUTPUT_DIR, "master_incidents.json")
    with open(master_file, "w", encoding="utf-8") as f:
        json.dump({
            'metadata': {
                'version': '1.0',
                'created_at': datetime.now().isoformat(),
                'total_incidents': len(merged),
                'total_relations': len(relations),
                'sources': [s['name'] for s in DATA_SOURCES],
            },
            'incidents': merged,
            'relations': relations,
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ë§ˆìŠ¤í„° ë°ì´í„° ì €ì¥: {master_file}")

    # TypeScript ë°ì´í„° íŒŒì¼ ìƒì„±
    ts_output = os.path.join(SCRIPTS_DIR, '..', 'lib', 'data.ts')

    # _source, _originalId ë“± ë‚´ë¶€ í•„ë“œ ì œê±°
    clean_incidents = []
    for inc in merged:
        clean = {k: v for k, v in inc.items() if not k.startswith('_')}
        clean_incidents.append(clean)

    ts_content = f'''// Auto-generated by merge_master_data.py
// Generated at: {datetime.now().isoformat()}
// Total incidents: {len(clean_incidents)}
// Total relations: {len(relations)}

import {{ IncidentsData }} from "./types";

export const incidentsData: IncidentsData = {json.dumps({
    'incidents': clean_incidents,
    'relations': relations,
}, ensure_ascii=False, indent=2)};
'''

    with open(ts_output, "w", encoding="utf-8") as f:
        f.write(ts_content)

    print(f"âœ… TypeScript ë°ì´í„° ì €ì¥: {ts_output}")

    # í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ìµœì¢… í†µê³„")
    print("=" * 60)

    # ì†ŒìŠ¤ë³„ í†µê³„
    source_stats = {}
    for inc in merged:
        src = inc.get('_source', 'Unknown')
        source_stats[src] = source_stats.get(src, 0) + 1

    print("\nì†ŒìŠ¤ë³„ ë¶„í¬:")
    for src, count in sorted(source_stats.items(), key=lambda x: -x[1]):
        print(f"   - {src}: {count}ê°œ")

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    cat_stats = {}
    for inc in merged:
        cat = inc.get('category', 'unknown')
        cat_stats[cat] = cat_stats.get(cat, 0) + 1

    print("\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for cat, count in sorted(cat_stats.items(), key=lambda x: -x[1]):
        print(f"   - {cat}: {count}ê°œ")

    # ì—°ë„ë³„ í†µê³„
    year_stats = {}
    for inc in merged:
        try:
            year = int(inc.get('date', '2000')[:4])
            decade = f"{(year // 10) * 10}s"
            year_stats[decade] = year_stats.get(decade, 0) + 1
        except:
            pass

    print("\nì—°ëŒ€ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ):")
    for decade, count in sorted(year_stats.items(), key=lambda x: -x[1])[:10]:
        print(f"   - {decade}: {count}ê°œ")


if __name__ == "__main__":
    main()
