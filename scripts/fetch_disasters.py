#!/usr/bin/env python3
"""
MHive ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
Wikipedia ì¹´í…Œê³ ë¦¬ì—ì„œ ì¬ë‚œ/ì‚¬ê±´ì‚¬ê³  ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import json
import requests
from datetime import datetime
from typing import List, Dict, Any, Optional, Set
from dataclasses import dataclass, asdict
from tqdm import tqdm
import re
import os
import time

# ê¸°ë³¸ ì„¤ì •
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
WIKI_API = "https://ko.wikipedia.org/w/api.php"
WIKI_EN_API = "https://en.wikipedia.org/w/api.php"
HEADERS = {
    'User-Agent': 'MHive/1.0 (https://github.com/Nicered/MHive; Data Collection Bot)'
}

# ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ ì •ì˜ (ì˜ì–´ Wikipedia)
CATEGORIES_CONFIG = {
    "disaster": {
        "categories": [
            "Category:Natural_disasters_by_year",
            "Category:21st-century_earthquakes",
            "Category:20th-century_earthquakes",
            "Category:Tsunamis",
            "Category:Floods",
            "Category:Volcanic_eruptions",
            "Category:Hurricanes",
            "Category:Typhoons",
        ],
        "max_depth": 2,
        "max_items": 100,
    },
    "accident": {
        "categories": [
            "Category:Aviation_accidents_and_incidents_by_year",
            "Category:Maritime_disasters",
            "Category:Nuclear_and_radiation_accidents_and_incidents",
            "Category:Bridge_disasters",
            "Category:Building_collapses",
            "Category:Industrial_disasters",
            "Category:Rail_accidents_and_incidents",
        ],
        "max_depth": 2,
        "max_items": 100,
    },
    "terrorism": {
        "categories": [
            "Category:Terrorist_incidents_by_year",
            "Category:Bombings",
            "Category:Mass_shootings",
        ],
        "max_depth": 2,
        "max_items": 60,
    },
    "crime": {
        "categories": [
            "Category:Massacres",
            "Category:Genocides",
            "Category:Serial_killers",
        ],
        "max_depth": 2,
        "max_items": 50,
    },
    "mystery": {
        "categories": [
            "Category:Unsolved_deaths",
            "Category:Unexplained_disappearances",
            "Category:Conspiracy_theories",
        ],
        "max_depth": 2,
        "max_items": 40,
    },
}


@dataclass
class Incident:
    id: int
    title: str
    category: str
    era: str
    date: str
    location: str
    summary: str
    description: str
    timeline: List[Dict[str, str]]
    theories: List[str]
    tags: List[str]
    sources: List[Dict[str, str]]
    relatedIncidents: List[int]
    images: List[str] = None
    casualties: Dict[str, int] = None
    coordinates: Dict[str, float] = None
    status: str = None
    originalTitle: str = None  # ì˜ì–´ ì›ì œëª©

    def __post_init__(self):
        if self.images is None:
            self.images = []
        if self.casualties is None:
            self.casualties = {}


def get_category_members(category: str, lang: str = "en", cmtype: str = "page|subcat") -> List[Dict]:
    """ì¹´í…Œê³ ë¦¬ì˜ ë©¤ë²„(ë¬¸ì„œ/í•˜ìœ„ì¹´í…Œê³ ë¦¬)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    api = WIKI_EN_API if lang == "en" else WIKI_API
    members = []
    cmcontinue = None

    while True:
        params = {
            "action": "query",
            "list": "categorymembers",
            "cmtitle": category,
            "cmlimit": "500",
            "cmtype": cmtype,
            "format": "json",
        }
        if cmcontinue:
            params["cmcontinue"] = cmcontinue

        try:
            response = requests.get(api, params=params, headers=HEADERS, timeout=15)
            data = response.json()
            members.extend(data.get("query", {}).get("categorymembers", []))

            if "continue" in data:
                cmcontinue = data["continue"].get("cmcontinue")
            else:
                break
        except Exception as e:
            print(f"Error fetching category {category}: {e}")
            break

    return members


def collect_pages_from_category(category: str, max_depth: int = 2, max_items: int = 100) -> Set[str]:
    """ì¹´í…Œê³ ë¦¬ì—ì„œ ì¬ê·€ì ìœ¼ë¡œ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    pages = set()
    visited_cats = set()

    def recurse(cat: str, depth: int):
        if depth > max_depth or cat in visited_cats or len(pages) >= max_items:
            return
        visited_cats.add(cat)

        members = get_category_members(cat)
        for m in members:
            if len(pages) >= max_items:
                break
            if m.get("ns") == 0:  # ì¼ë°˜ ë¬¸ì„œ
                pages.add(m["title"])
            elif m.get("ns") == 14 and depth < max_depth:  # ì¹´í…Œê³ ë¦¬
                recurse(m["title"], depth + 1)

    recurse(category, 0)
    return pages


def get_wikipedia_page(title: str, lang: str = "en") -> Optional[Dict]:
    """Wikipedia APIì—ì„œ í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    api = WIKI_EN_API if lang == "en" else WIKI_API

    params = {
        "action": "query",
        "titles": title,
        "prop": "extracts|categories|coordinates|pageimages|langlinks",
        "exintro": False,
        "explaintext": True,
        "format": "json",
        "piprop": "original",
        "lllang": "ko" if lang == "en" else "en",
        "lllimit": "1",
    }

    try:
        response = requests.get(api, params=params, headers=HEADERS, timeout=15)
        data = response.json()
        pages = data.get("query", {}).get("pages", {})

        for page_id, page_data in pages.items():
            if page_id != "-1":
                return page_data
    except Exception as e:
        print(f"Error fetching {title}: {e}")

    return None


def extract_date_from_text(text: str, title: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ì œëª©ì—ì„œ ì—°ë„ ì¶”ì¶œ ì‹œë„
    year_match = re.search(r'(19\d{2}|20\d{2})', title)

    # í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
    date_patterns = [
        r'(\d{1,2})\s+(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{4})',
        r'(January|February|March|April|May|June|July|August|September|October|November|December)\s+(\d{1,2}),?\s+(\d{4})',
        r'(\d{4})-(\d{2})-(\d{2})',
    ]

    months = {
        'January': '01', 'February': '02', 'March': '03', 'April': '04',
        'May': '05', 'June': '06', 'July': '07', 'August': '08',
        'September': '09', 'October': '10', 'November': '11', 'December': '12'
    }

    for pattern in date_patterns:
        match = re.search(pattern, text[:1000])
        if match:
            groups = match.groups()
            if len(groups) == 3:
                if groups[1] in months:
                    return f"{groups[2]}-{months[groups[1]]}-{int(groups[0]):02d}"
                elif groups[0] in months:
                    return f"{groups[2]}-{months[groups[0]]}-{int(groups[1]):02d}"
                else:
                    return f"{groups[0]}-{groups[1]}-{groups[2]}"

    if year_match:
        return f"{year_match.group(1)}-01-01"

    return "2000-01-01"


def extract_location(text: str, title: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ì¼ë°˜ì ì¸ ìœ„ì¹˜ íŒ¨í„´
    location_patterns = [
        r'(?:in|at|near)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*),?\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)',
        r'(?:occurred|happened|struck|hit)\s+(?:in|at)\s+([A-Z][a-zA-Z\s,]+)',
    ]

    for pattern in location_patterns:
        match = re.search(pattern, text[:500])
        if match:
            return match.group(1).strip()[:50]

    # ì œëª©ì—ì„œ ìœ„ì¹˜ ì¶”ì¶œ
    location_in_title = re.search(r'(?:in|at)\s+([A-Z][a-zA-Z\s]+)', title)
    if location_in_title:
        return location_in_title.group(1).strip()[:50]

    return "Unknown"


def determine_era(date_str: str) -> str:
    """ë‚ ì§œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œëŒ€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    try:
        year = int(date_str.split("-")[0])
        if year < 0:
            return "ancient"
        elif year < 1900:
            return "modern"
        else:
            return "contemporary"
    except:
        return "contemporary"


def extract_casualties(text: str) -> Dict[str, int]:
    """í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìƒì ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    casualties = {}

    patterns = {
        "deaths": [
            r'(\d{1,7})\s*(?:people\s+)?(?:were\s+)?(?:killed|died|dead|death|deaths|fatalities)',
            r'(?:killed|death toll|fatalities)[:\s]+(\d{1,7})',
            r'(\d{1,7})\s+(?:dead|killed)',
        ],
        "injuries": [
            r'(\d{1,7})\s*(?:people\s+)?(?:were\s+)?(?:injured|wounded|hurt)',
            r'(?:injured|wounded)[:\s]+(\d{1,7})',
        ],
        "missing": [
            r'(\d{1,7})\s*(?:people\s+)?(?:were\s+)?(?:missing|disappeared)',
            r'(?:missing)[:\s]+(\d{1,7})',
        ],
    }

    for key, pattern_list in patterns.items():
        for pattern in pattern_list:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    num = int(match.group(1).replace(",", ""))
                    if num > 0 and num < 10000000:  # í•©ë¦¬ì ì¸ ë²”ìœ„
                        casualties[key] = num
                        break
                except:
                    pass

    return casualties


def extract_tags(text: str, category: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ íƒœê·¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    tags = []

    # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ íƒœê·¸
    category_tags = {
        "disaster": ["ì¬ë‚œ", "ìì—°ì¬í•´"],
        "terrorism": ["í…ŒëŸ¬"],
        "accident": ["ì‚¬ê³ "],
        "crime": ["ë²”ì£„"],
        "mystery": ["ë¯¸ìŠ¤í„°ë¦¬"],
        "conspiracy": ["ìŒëª¨ë¡ "],
        "unsolved": ["ë¯¸ì œì‚¬ê±´"],
    }
    tags.extend(category_tags.get(category, []))

    # í‚¤ì›Œë“œ ê¸°ë°˜ íƒœê·¸
    keyword_tags = {
        "earthquake": "ì§€ì§„",
        "tsunami": "ì“°ë‚˜ë¯¸",
        "hurricane": "í—ˆë¦¬ì¼€ì¸",
        "typhoon": "íƒœí’",
        "flood": "í™ìˆ˜",
        "fire": "í™”ì¬",
        "explosion": "í­ë°œ",
        "aircraft": "í•­ê³µì‚¬ê³ ",
        "plane": "í•­ê³µì‚¬ê³ ",
        "ship": "í•´ìƒì‚¬ê³ ",
        "train": "ì² ë„ì‚¬ê³ ",
        "nuclear": "í•µ/ì›ì „",
        "bomb": "í­íƒ„",
        "shooting": "ì´ê¸°",
        "massacre": "í•™ì‚´",
        "serial killer": "ì—°ì‡„ì‚´ì¸",
    }

    text_lower = text.lower()
    for keyword, tag in keyword_tags.items():
        if keyword in text_lower:
            tags.append(tag)

    return list(set(tags))[:10]


def process_page(title: str, incident_id: int, category: str) -> Optional[Incident]:
    """Wikipedia í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•˜ì—¬ Incident ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # ì˜ì–´ Wikipediaì—ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    page_data = get_wikipedia_page(title, "en")
    if not page_data:
        return None

    extract = page_data.get("extract", "")
    if not extract or len(extract) < 100:
        return None

    # í•œêµ­ì–´ Wikipedia ì—°ê²° í™•ì¸
    langlinks = page_data.get("langlinks", [])
    ko_title = None
    for ll in langlinks:
        if ll.get("lang") == "ko":
            ko_title = ll.get("*")
            break

    # í•œêµ­ì–´ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ í•œêµ­ì–´ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    if ko_title:
        ko_page = get_wikipedia_page(ko_title, "ko")
        if ko_page and ko_page.get("extract"):
            display_title = ko_title
            display_extract = ko_page.get("extract", "")
        else:
            display_title = title
            display_extract = extract
    else:
        display_title = title
        display_extract = extract

    # ë‚ ì§œ ì¶”ì¶œ
    date = extract_date_from_text(extract, title)

    # ìœ„ì¹˜ ì¶”ì¶œ
    location = extract_location(extract, title)

    # ì¢Œí‘œ
    coordinates = None
    if "coordinates" in page_data:
        coord = page_data["coordinates"][0]
        coordinates = {"lat": coord["lat"], "lng": coord["lon"]}

    # ìš”ì•½ (ì²« 2-3ë¬¸ì¥)
    sentences = display_extract.split(". ")
    summary = ". ".join(sentences[:3]) + "." if sentences else display_extract[:300]

    # ìƒì„¸ ì„¤ëª…
    paragraphs = display_extract.split("\n\n")
    description = "\n\n".join(p.strip() for p in paragraphs[:5] if p.strip())

    # ì‚¬ìƒì ì¶”ì¶œ
    casualties = extract_casualties(extract)

    # íƒœê·¸ ì¶”ì¶œ
    tags = extract_tags(extract, category)

    # ì†ŒìŠ¤
    sources = [{"name": "Wikipedia", "url": f"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}"}]
    if ko_title:
        sources.append({"name": "Wikipedia (KO)", "url": f"https://ko.wikipedia.org/wiki/{ko_title.replace(' ', '_')}"})

    return Incident(
        id=incident_id,
        title=display_title,
        category=category,
        era=determine_era(date),
        date=date,
        location=location,
        summary=summary[:400],
        description=description[:3000],
        timeline=[],
        theories=[],
        tags=tags,
        sources=sources,
        relatedIncidents=[],
        images=[],
        casualties=casualties if casualties else None,
        coordinates=coordinates,
        status="resolved",
        originalTitle=title if title != display_title else None,
    )


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ” MHive ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    print("   Wikipedia ì¹´í…Œê³ ë¦¬ì—ì„œ ìë™ ìˆ˜ì§‘")

    all_pages = {}  # {page_title: category}

    # ê° ì¹´í…Œê³ ë¦¬ì—ì„œ í˜ì´ì§€ ìˆ˜ì§‘
    for category, config in CATEGORIES_CONFIG.items():
        print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘: {category}")
        category_pages = set()

        for cat in config["categories"]:
            if len(category_pages) >= config["max_items"]:
                break
            pages = collect_pages_from_category(
                cat,
                max_depth=config["max_depth"],
                max_items=config["max_items"] - len(category_pages)
            )
            category_pages.update(pages)
            print(f"   - {cat}: {len(pages)}ê°œ ìˆ˜ì§‘")

        # ê¸°ì¡´ì— ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í˜ì´ì§€ë§Œ ì¶”ê°€
        for page in category_pages:
            if page not in all_pages:
                all_pages[page] = category

        print(f"   ì´ {len(category_pages)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")

    print(f"\nğŸ“Š ì´ ìˆ˜ì§‘ ëŒ€ìƒ: {len(all_pages)}ê°œ ë¬¸ì„œ")

    # ê° í˜ì´ì§€ ì²˜ë¦¬
    incidents = []
    incident_id = 1

    for title, category in tqdm(all_pages.items(), desc="ì²˜ë¦¬ ì¤‘"):
        incident = process_page(title, incident_id, category)
        if incident:
            incidents.append(asdict(incident))
            incident_id += 1
        time.sleep(0.1)  # API ìš”ì²­ ì œí•œ

    # ê²°ê³¼ ì €ì¥
    output_file = os.path.join(OUTPUT_DIR, "incidents_raw.json")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({"incidents": incidents}, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì™„ë£Œ! {len(incidents)}ê°œ ì‚¬ê±´ ì €ì¥ë¨")
    print(f"   ì¶œë ¥ íŒŒì¼: {output_file}")

    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    cat_stats = {}
    for inc in incidents:
        cat = inc["category"]
        cat_stats[cat] = cat_stats.get(cat, 0) + 1

    print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for cat, count in sorted(cat_stats.items(), key=lambda x: -x[1]):
        print(f"   - {cat}: {count}ê°œ")


if __name__ == "__main__":
    main()
