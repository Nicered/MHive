#!/usr/bin/env python3
"""
Harvard LIL Cold Cases ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ë¯¸êµ­ ë²•ì› í˜•ì‚¬ ì‚¬ê±´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import json
import os
from typing import List, Dict, Optional
from datetime import datetime
from tqdm import tqdm

OUTPUT_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'sources')

# ì£¼ìš” í˜•ì‚¬ ì‚¬ê±´ í‚¤ì›Œë“œ
CRIME_KEYWORDS = [
    'murder', 'homicide', 'killing', 'manslaughter',
    'kidnapping', 'abduction', 'missing',
    'serial', 'massacre', 'genocide',
    'terrorism', 'terrorist', 'bombing',
    'assault', 'robbery', 'burglary',
    'rape', 'sexual assault',
    'arson', 'fraud', 'conspiracy',
]


def extract_crime_type(text: str) -> tuple:
    """í…ìŠ¤íŠ¸ì—ì„œ ë²”ì£„ ìœ í˜•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    text_lower = text.lower()

    crime_map = {
        ('murder', 'homicide', 'killing', 'manslaughter'): ('crime', 'ì‚´ì¸'),
        ('kidnapping', 'abduction'): ('crime', 'ë‚©ì¹˜'),
        ('terrorism', 'terrorist', 'bombing'): ('terrorism', 'í…ŒëŸ¬'),
        ('massacre', 'genocide'): ('crime', 'í•™ì‚´'),
        ('serial',): ('crime', 'ì—°ì‡„ë²”ì£„'),
        ('rape', 'sexual assault'): ('crime', 'ì„±ë²”ì£„'),
        ('robbery', 'burglary'): ('crime', 'ê°•ë„'),
        ('arson',): ('crime', 'ë°©í™”'),
        ('fraud',): ('crime', 'ì‚¬ê¸°'),
        ('assault',): ('crime', 'í­í–‰'),
    }

    for keywords, (category, tag) in crime_map.items():
        for kw in keywords:
            if kw in text_lower:
                return category, tag

    return 'crime', 'ë²”ì£„'


def is_significant_case(item: Dict) -> bool:
    """ì¤‘ìš”í•œ í˜•ì‚¬ ì‚¬ê±´ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    # ì¸ìš© íšŸìˆ˜ê°€ ë†’ì€ ì‚¬ê±´
    citation_count = item.get('citation_count', 0)
    if citation_count >= 50:
        return True

    # í˜•ì‚¬ ë²•ì›ì¸ ê²½ìš°
    court_name = (item.get('court_full_name') or '').lower()
    if 'criminal' in court_name:
        return True

    # ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš°
    case_name = (item.get('case_name_full') or item.get('case_name') or '').lower()
    opinions = item.get('opinions', [])
    opinion_text = ' '.join([op.get('opinion_text', '')[:1000] for op in opinions]).lower()

    full_text = case_name + ' ' + opinion_text
    for kw in CRIME_KEYWORDS:
        if kw in full_text:
            return True

    return False


def transform_to_incident(item: Dict, incident_id: int) -> Dict:
    """ë²•ì› ë°ì´í„°ë¥¼ MHive Incident í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    case_name = item.get('case_name_full') or item.get('case_name') or 'Unknown Case'
    case_name_short = item.get('case_name_short') or item.get('case_name') or 'Unknown'

    date_filed = item.get('date_filed', '')
    jurisdiction = item.get('court_jurisdiction', 'USA')
    court_name = item.get('court_full_name') or item.get('court_short_name') or 'Unknown Court'

    # ë²”ì£„ ìœ í˜• ì¶”ì¶œ
    opinions = item.get('opinions', [])
    opinion_text = ' '.join([op.get('opinion_text', '')[:2000] for op in opinions])
    category, crime_tag = extract_crime_type(case_name + ' ' + opinion_text)

    # ì œëª©
    title = f"{case_name_short} ì‚¬ê±´"
    if date_filed:
        year = date_filed[:4]
        title = f"{year}ë…„ {case_name_short} ì‚¬ê±´"

    # ìœ„ì¹˜
    location = f"{jurisdiction}, USA"

    # ìš”ì•½
    summary = f"ë¯¸êµ­ {jurisdiction} ë²•ì› íŒê²° ì‚¬ê±´. "
    summary += f"ì‚¬ê±´ëª…: {case_name_short}. "
    summary += f"íŒê²°ì¼: {date_filed}. "
    summary += f"ë²•ì›: {court_name}."

    # ìƒì„¸ ì„¤ëª…
    description = f"## ì‚¬ê±´ ì •ë³´\n\n"
    description += f"**ì‚¬ê±´ëª…**: {case_name}\n"
    description += f"**íŒê²°ì¼**: {date_filed}\n"
    description += f"**ë²•ì›**: {court_name}\n"
    description += f"**ê´€í• **: {jurisdiction}\n\n"

    if item.get('judges'):
        description += f"**íŒì‚¬**: {item.get('judges')}\n\n"

    if item.get('attorneys'):
        description += f"## ë³€í˜¸ì¸\n\n{item.get('attorneys')[:500]}\n\n"

    citations = item.get('citations', [])
    if citations:
        description += f"## ì¸ìš©\n\n"
        for cite in citations[:5]:
            description += f"- {cite}\n"

    # íŒê²°ë¬¸ ìš”ì•½ (ìˆëŠ” ê²½ìš°)
    if opinions and opinions[0].get('opinion_text'):
        opinion = opinions[0]['opinion_text'][:1500]
        description += f"\n## íŒê²°ë¬¸ ìš”ì•½\n\n{opinion}..."

    # íƒœê·¸
    tags = ['ë²”ì£„', 'ë²•ì›íŒê²°', 'ë¯¸êµ­', crime_tag]
    if item.get('precedential_status') == 'Published':
        tags.append('íŒë¡€')
    if item.get('citation_count', 0) >= 100:
        tags.append('ì£¼ìš”íŒê²°')

    # ì†ŒìŠ¤
    citations_str = item.get('citations', [''])[0] if item.get('citations') else ''
    source_url = f"https://case.law/search?q={item.get('slug', '')}"

    return {
        'id': incident_id,
        'title': title,
        'category': category,
        'era': 'contemporary',
        'date': date_filed,
        'location': location,
        'summary': summary[:400],
        'description': description[:3000],
        'timeline': [],
        'theories': [],
        'tags': list(set(tags)),
        'sources': [{
            'name': 'Harvard Law - Case Law Access Project',
            'url': source_url
        }],
        'relatedIncidents': [],
        'images': [],
        'casualties': None,
        'coordinates': None,
        'status': 'resolved',
        'originalId': item.get('id'),
        'citationCount': item.get('citation_count', 0),
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("âš–ï¸ Harvard LIL Cold Cases ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")

    try:
        from datasets import load_dataset
    except ImportError:
        print("âŒ datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install datasets")
        return

    # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ë°ì´í„° ë¡œë“œ
    print("   ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ (ìŠ¤íŠ¸ë¦¬ë°)...")
    ds = load_dataset('harvard-lil/cold-cases', split='train', streaming=True)

    # ì¤‘ìš” ì‚¬ê±´ë§Œ í•„í„°ë§í•˜ë©° ìˆ˜ì§‘
    significant_cases = []
    total_scanned = 0
    max_scan = 50000  # ìµœëŒ€ ìŠ¤ìº” ìˆ˜
    max_collect = 1000  # ìµœëŒ€ ìˆ˜ì§‘ ìˆ˜

    print(f"   ìµœëŒ€ {max_scan}ê°œ ìŠ¤ìº”, {max_collect}ê°œ ìˆ˜ì§‘ ëª©í‘œ")

    with tqdm(total=max_scan, desc="ìŠ¤ìº” ì¤‘") as pbar:
        for item in ds:
            total_scanned += 1
            pbar.update(1)

            if is_significant_case(item):
                significant_cases.append(item)
                pbar.set_postfix({'ìˆ˜ì§‘': len(significant_cases)})

            if total_scanned >= max_scan or len(significant_cases) >= max_collect:
                break

    print(f"\nğŸ“Š ì´ ìŠ¤ìº”: {total_scanned}ê°œ, ìˆ˜ì§‘: {len(significant_cases)}ê°œ")

    # MHive í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    incidents = []
    for i, item in enumerate(tqdm(significant_cases, desc="ë°ì´í„° ë³€í™˜")):
        incident = transform_to_incident(item, i + 1)
        incidents.append(incident)

    # ì¸ìš© íšŸìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    incidents.sort(key=lambda x: x.get('citationCount', 0), reverse=True)

    # ID ì¬í• ë‹¹
    for i, inc in enumerate(incidents):
        inc['id'] = i + 1

    # ê²°ê³¼ ì €ì¥
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_file = os.path.join(OUTPUT_DIR, "coldcases_crimes.json")

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            'source': 'Harvard LIL Cold Cases',
            'collected_at': datetime.now().isoformat(),
            'total_count': len(incidents),
            'incidents': incidents
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì™„ë£Œ! {len(incidents)}ê°œ ì‚¬ê±´ ì €ì¥ë¨")
    print(f"   ì¶œë ¥ íŒŒì¼: {output_file}")

    # í†µê³„
    cat_stats = {}
    for inc in incidents:
        cat = inc.get('category', 'unknown')
        cat_stats[cat] = cat_stats.get(cat, 0) + 1

    print("\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
    for cat, count in sorted(cat_stats.items(), key=lambda x: -x[1]):
        print(f"   - {cat}: {count}ê°œ")


if __name__ == "__main__":
    main()
