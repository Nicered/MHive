#!/usr/bin/env python3
"""
NOAA ì“°ë‚˜ë¯¸ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ì „ ì„¸ê³„ ì“°ë‚˜ë¯¸ ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import json
import requests
from typing import List, Dict, Optional
from tqdm import tqdm
import os
from datetime import datetime

OUTPUT_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'sources')
NOAA_TSUNAMI_API = "https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/tsunamis/events"

# ì“°ë‚˜ë¯¸ ì›ì¸ ì½”ë“œ ë§¤í•‘
CAUSE_CODES = {
    1: 'ì§€ì§„ (Earthquake)',
    2: 'ì˜ì‹¬ë˜ëŠ” ì§€ì§„ (Questionable Earthquake)',
    3: 'ì§€ì§„ ë° ì‚°ì‚¬íƒœ (Earthquake and Landslide)',
    4: 'í™”ì‚° ë° ì§€ì§„ (Volcano and Earthquake)',
    5: 'í™”ì‚°, ì§€ì§„ ë° ì‚°ì‚¬íƒœ (Volcano, Earthquake, and Landslide)',
    6: 'í™”ì‚° (Volcano)',
    7: 'í™”ì‚° ë° ì‚°ì‚¬íƒœ (Volcano and Landslide)',
    8: 'ì‚°ì‚¬íƒœ (Landslide)',
    9: 'ê¸°ìƒ (Meteorological)',
    10: 'í­ë°œ (Explosion)',
    11: 'ì²œë¬¸í•™ì  ì¡°ë¥˜ (Astronomical Tide)',
    12: 'ì•Œ ìˆ˜ ì—†ìŒ (Unknown)',
}

# ìœ íš¨ì„± ì½”ë“œ ë§¤í•‘
VALIDITY_CODES = {
    -1: 'ì˜ëª»ëœ ì´ë²¤íŠ¸',
    0: 'ê°€ëŠ¥ì„± ë‚®ìŒ',
    1: 'ì˜ì‹¬ìŠ¤ëŸ¬ì›€',
    2: 'ë¶„ì„ í•„ìš”',
    3: 'ê°€ëŠ¥ì„± ìˆìŒ',
    4: 'í™•ì¸ë¨',
}


def fetch_tsunamis(min_year: int = 1900, max_year: int = None) -> List[Dict]:
    """NOAA APIì—ì„œ ì“°ë‚˜ë¯¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    if max_year is None:
        max_year = datetime.now().year

    all_items = []
    page = 1
    max_results = 500

    while True:
        params = {
            'minYear': min_year,
            'maxYear': max_year,
            'maxResults': max_results,
            'page': page,
        }

        try:
            response = requests.get(NOAA_TSUNAMI_API, params=params, timeout=30)
            response.raise_for_status()
            data = response.json()

            items = data.get('items', [])
            all_items.extend(items)

            total_pages = data.get('totalPages', 1)
            if page >= total_pages:
                break
            page += 1
        except Exception as e:
            print(f"Error fetching tsunamis (page {page}): {e}")
            break

    return all_items


def format_date(year: int, month: Optional[int], day: Optional[int]) -> str:
    """ì—°ë„, ì›”, ì¼ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not year:
        return ""
    month = month or 1
    day = day or 1
    return f"{year:04d}-{month:02d}-{day:02d}"


def format_time(hour: Optional[int], minute: Optional[int]) -> str:
    """ì‹œê°„ì„ HH:MM í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if hour is None:
        return ""
    minute = minute or 0
    return f"{hour:02d}:{minute:02d}"


def get_wave_severity(max_height: Optional[float]) -> str:
    """íŒŒê³ ì— ë”°ë¥¸ ì‹¬ê°ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not max_height:
        return "ì •ë³´ ì—†ìŒ"
    if max_height >= 10:
        return "ë§¤ìš° ë†’ìŒ (10m+)"
    elif max_height >= 5:
        return "ë†’ìŒ (5-10m)"
    elif max_height >= 2:
        return "ì¤‘ê°„ (2-5m)"
    elif max_height >= 0.5:
        return "ë‚®ìŒ (0.5-2m)"
    else:
        return "ë§¤ìš° ë‚®ìŒ (<0.5m)"


def determine_era(year: int) -> str:
    """ì—°ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œëŒ€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    if year < 0:
        return "ancient"
    elif year < 1900:
        return "modern"
    else:
        return "contemporary"


def transform_to_incident(item: Dict, incident_id: int) -> Dict:
    """NOAA ë°ì´í„°ë¥¼ MHive Incident í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    year = item.get('year', 0)
    month = item.get('month')
    day = item.get('day')
    hour = item.get('hour')
    minute = item.get('minute')

    date = format_date(year, month, day)
    time_str = format_time(hour, minute)

    country = item.get('country', 'Unknown')
    location_name = item.get('locationName', '')
    location = f"{location_name}, {country}" if location_name else country

    cause_code = item.get('causeCode', 12)
    cause = CAUSE_CODES.get(cause_code, 'ì•Œ ìˆ˜ ì—†ìŒ')

    max_height = item.get('maxWaterHeight')
    wave_severity = get_wave_severity(max_height)

    validity = item.get('eventValidity', 0)
    validity_str = VALIDITY_CODES.get(validity, 'ì•Œ ìˆ˜ ì—†ìŒ')

    # ì œëª© ìƒì„±
    title = f"{year}ë…„ {country} ì“°ë‚˜ë¯¸"
    if location_name:
        title = f"{year}ë…„ {location_name} ì“°ë‚˜ë¯¸"

    # í”¼í•´ ì •ë³´
    deaths = item.get('deaths') or item.get('deathsTotal')
    injuries = item.get('injuries') or item.get('injuriesTotal')
    missing = item.get('missing') or item.get('missingTotal')
    houses_destroyed = item.get('housesDestroyed') or item.get('housesDestroyedTotal')

    # ìš”ì•½
    summary = f"{year}ë…„ {location}ì—ì„œ ë°œìƒí•œ ì“°ë‚˜ë¯¸. "
    summary += f"ì›ì¸: {cause}. "
    if max_height:
        summary += f"ìµœëŒ€ íŒŒê³ : {max_height:.1f}m. "
    if deaths:
        summary += f"ì‚¬ë§ì: {deaths}ëª…."

    # ìƒì„¸ ì„¤ëª…
    description = f"## ì“°ë‚˜ë¯¸ ì •ë³´\n\n"
    description += f"**ë°œìƒ ì¼ì‹œ**: {date}"
    if time_str:
        description += f" {time_str} UTC"
    description += f"\n**ìœ„ì¹˜**: {location}\n"
    description += f"**ì›ì¸**: {cause}\n"
    description += f"**ì´ë²¤íŠ¸ ìœ íš¨ì„±**: {validity_str}\n"

    if max_height:
        description += f"\n## íŒŒë„ ì •ë³´\n\n"
        description += f"**ìµœëŒ€ íŒŒê³ **: {max_height:.1f}m ({wave_severity})\n"

    num_runups = item.get('numRunups', 0)
    if num_runups:
        description += f"**ì¸¡ì • ì§€ì  ìˆ˜**: {num_runups}ê°œ\n"

    if deaths or injuries or missing or houses_destroyed:
        description += f"\n## í”¼í•´ í˜„í™©\n\n"
        if deaths:
            description += f"- ì‚¬ë§ì: {deaths}ëª…\n"
        if injuries:
            description += f"- ë¶€ìƒì: {injuries}ëª…\n"
        if missing:
            description += f"- ì‹¤ì¢…ì: {missing}ëª…\n"
        if houses_destroyed:
            description += f"- íŒŒê´´ëœ ê°€ì˜¥: {houses_destroyed}ì±„\n"

    # ì‚¬ìƒì ì •ë³´
    casualties = {}
    if deaths:
        casualties['deaths'] = deaths
    if injuries:
        casualties['injuries'] = injuries
    if missing:
        casualties['missing'] = missing

    # íƒœê·¸
    tags = ['ì¬ë‚œ', 'ìì—°ì¬í•´', 'ì“°ë‚˜ë¯¸']
    if cause_code in [1, 2, 3]:
        tags.append('ì§€ì§„')
    if cause_code in [4, 5, 6, 7]:
        tags.append('í™”ì‚°')
    if cause_code == 8:
        tags.append('ì‚°ì‚¬íƒœ')
    if max_height and max_height >= 5:
        tags.append('ëŒ€ê·œëª¨')
    if deaths and deaths >= 100:
        tags.append('ëŒ€í˜•í”¼í•´')
    if item.get('oceanicTsunami'):
        tags.append('ì›ì–‘ì“°ë‚˜ë¯¸')

    # ì¢Œí‘œ
    lat = item.get('latitude')
    lng = item.get('longitude')
    coordinates = {'lat': lat, 'lng': lng} if lat and lng else None

    return {
        'id': incident_id,
        'title': title,
        'category': 'disaster',
        'era': determine_era(year),
        'date': date,
        'location': location,
        'summary': summary[:400],
        'description': description[:3000],
        'timeline': [],
        'theories': [],
        'tags': list(set(tags)),
        'sources': [{
            'name': 'NOAA NGDC',
            'url': f"https://www.ngdc.noaa.gov/hazel/view/hazards/tsunami/event-more-info/{item.get('id')}"
        }],
        'relatedIncidents': [],
        'images': [],
        'casualties': casualties if casualties else None,
        'coordinates': coordinates,
        'status': 'resolved',
        'originalId': item.get('id'),
        'maxWaveHeight': max_height,
        'causeCode': cause_code,
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸŒŠ NOAA ì“°ë‚˜ë¯¸ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")

    # 1900ë…„ë¶€í„° í˜„ì¬ê¹Œì§€ ë°ì´í„° ìˆ˜ì§‘
    items = fetch_tsunamis(min_year=1900)

    print(f"\nğŸ“Š ì´ {len(items)}ê°œ ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘")

    # í™•ì¸ëœ ì´ë²¤íŠ¸ë§Œ í•„í„°ë§ (validity >= 3)
    valid_items = [item for item in items if item.get('eventValidity', 0) >= 3]
    print(f"   í™•ì¸ëœ ì´ë²¤íŠ¸ (validity >= 3): {len(valid_items)}ê°œ")

    # MHive í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    incidents = []
    for i, item in enumerate(tqdm(valid_items, desc="ë°ì´í„° ë³€í™˜")):
        incident = transform_to_incident(item, i + 1)
        incidents.append(incident)

    # ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    incidents.sort(key=lambda x: x.get('date', ''), reverse=True)

    # ID ì¬í• ë‹¹
    for i, inc in enumerate(incidents):
        inc['id'] = i + 1

    # ê²°ê³¼ ì €ì¥
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    output_file = os.path.join(OUTPUT_DIR, "noaa_tsunamis.json")

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            'source': 'NOAA NGDC Tsunami Database',
            'collected_at': datetime.now().isoformat(),
            'total_count': len(incidents),
            'incidents': incidents
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ì™„ë£Œ! {len(incidents)}ê°œ ì‚¬ê±´ ì €ì¥ë¨")
    print(f"   ì¶œë ¥ íŒŒì¼: {output_file}")

    # ì›ì¸ë³„ í†µê³„
    cause_stats = {}
    for inc in incidents:
        cause = CAUSE_CODES.get(inc.get('causeCode', 12), 'ê¸°íƒ€')
        cause_stats[cause] = cause_stats.get(cause, 0) + 1

    print("\nğŸ“ˆ ì›ì¸ë³„ ë¶„í¬:")
    for cause, count in sorted(cause_stats.items(), key=lambda x: -x[1])[:5]:
        print(f"   - {cause}: {count}ê°œ")

    # ì—°ëŒ€ë³„ í†µê³„
    decade_stats = {}
    for inc in incidents:
        try:
            year = int(inc.get('date', '2000')[:4])
            decade = f"{(year // 10) * 10}s"
            decade_stats[decade] = decade_stats.get(decade, 0) + 1
        except:
            pass

    print("\nğŸ“… ì—°ëŒ€ë³„ ë¶„í¬ (ìƒìœ„ 5ê°œ):")
    for decade, count in sorted(decade_stats.items(), key=lambda x: -x[1])[:5]:
        print(f"   - {decade}: {count}ê°œ")


if __name__ == "__main__":
    main()
